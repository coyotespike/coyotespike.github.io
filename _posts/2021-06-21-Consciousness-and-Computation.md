---
layout: post
title: Machines and Consciousness
date: 2021-06-21
# Should be longer than 20 words, so it will appear as post summary
description: Could we really act human without also being awake?
# tags will also be used as html meta keywords. Only use one, as it goes in the URL
categories: ["philosophy"]

show_meta: true
comments: true
mathjax: true
gistembed: true
published: true
noindex: false
nofollow: false
---

## The Evolution of Zombies

A disturbing number of thinkpieces on whether AIs will ever be conscious start off, explicitly or implicity, with something like:

> of course, all of us humans could do everything we do with zero consciousness, so it's hard to explain why we have it. Machines are just that, machines, and no matter what human tasks they take over, what feats they match, they will always remain wires and electricity, nothing more.

Beyond thinkpiece writers, my impression is that most people think this way too.

Yet all available evidence undermines the unquestioned assumption that humans could operate without consciousness. Every single functional example of high general-purpose intelligence that we know of exhibits consciousness, and the more intelligence, the more consciousness. By this I mean our fellow animals.

After all, by the p-zombie argument, some of the other mammals might have failed to develop consciousness. Dogs might have developed as efficient hunting machines, herding robots, guarding functionaries, with no emotions or will of their own on display. Indeed Descartes said dogs did not have a soul, and therefore are not conscious. Vivisect a dog without anesthesia, he said, its howls and whining are mere mechanical responses. 

Of course we cannot _know_ for sure that dogs are awake, just as we cannot know about other humans, but there's no justification for believing either humans or dogs lack consciousness. Anyone who has lived with a dog and experienced its full personality knows that.

Mammals are an easy case, but take other examples, say birds. Even in birds, the more intelligent ones don't merely accomplish their goals with greater ease. An African Grey parrot has more _opinions_, and expresses a greater range of feelings, than does a starling (which is certainly full of opinions as it is).

Perhaps evolution can develop an intelligent agent without consciousness, but so far it never has, or not that we can tell. There is not the faintest hint anywhere in nature that consciousness is some purely optional and hence irreducibly mysterious add-on to agentic intelligence. To all appearances, the two have always gone hand-in-hand.

Now, it is easy to _imagine_ a p-zombie (perhaps that is why they are so popular as a foundation for an argument) but that says nothing about whether they are actually possible. Nothing I have said so far rules them out; you can hold to your belief in the possibility of p-zombies if you are determined to. Still, before you carry on deducing what will and won't happen with machines, you should bring some empirical support to the p-zombie assumption.

## A Very Stupid Genius

True, our current AI programs do demonstrate that intelligence and computation are not matched one-to-one. AlphaGo is much more intelligent in its domain than any human, and in a game that most experts thought would take decades more for a computer to match a human.

The general rule is that if something can only be done by a human, it represents our uniqueness, our ineffable soul, the _je ne sais quois_ of the human spirit, while if a machine can do it then it's mere calculation. Chess was a marvel of the lucidity of human reason, deviousness and will and guts on display, until the machines outclassed our best. Now Hollywood hopes that stories and music make us special.

In contrast to the general rule, I think AlphaGo and GPT-3 do embody real intelligence, in a way that tells us something about humans, yet they aren't awake. Nevertheless, this separation of intelligence and consciousness tells us almost nothing about the relation of higher intelligence and consciousness, because even these AIs are massively dumb. 

They have been purpose-built to do exactly one thing in all the world. They excel at one minute slice of intelligence, one nearly incomprehensibly tiny piece of intelligence. They cannot play chess, and also play music, and dance, and do math, and wash dishes, and tie their shoes. Let alone much more difficult tasks such as making someone else angry or happy, or scheming to get their way in a group! [1](gtv0_LKti)

We humans have as many neurons as there are stars in the Milky Way, and nearly as many connections between those neurons as there are stars in the universe. These AIs' neuronal complexity is pitiful. They do not have the brain of a snail, let alone a bird. Of course they don't have consciousness. To think these AIs demonstrate that computers will never wake up is like pointing at a paper airplane and proclaiming space flight impossible.

## Unknown Unknowns are Hard

I certainly am not saying consciousness is not a hard problem. It is hard, hard and mysterious. The more we pick apart the brain, the more we fail to find the source of consciousness. It isn't in any part, or in any identifiable collection of parts, and we can't figure out how or whether it arises as an epiphenomenon from some _gestalt_ of the brain. We don't know the most important thing about ourselves: why we are awake.

This [fundamental ignorance](https://www.amazon.com/Ignorance-Drives-Science-Stuart-Firestein/dp/0199828075), despite all our science, must be our starting point for any speculating and philosophizing, but we do have some evidence to point us in the right direction.

We can start off, again, with biology. In general, the larger the animal, the larger the brain needed to run the body. If you want to know how intelligent an animal is, as a very rough rule you simply look at how much extra brain it has left over: the [ratio of brain to body](https://en.wikipedia.org/wiki/Encephalization_quotient). Actually you must adjust and count the number of neurons in the forebrain, but then you'll find, as expected, monkeys, elephants, dolphins much higher than most animals, and we humans up in a league of our own. Intelligence, computing power, and consciousness go together.

While some have gotten desperate for an explanation for consciousness and attempted to locate it in matter itself (panpsychism), it's remarkable that consciousness only reveals itself in big clumps of matter which always happen to be arranged in a Turing-complete manner. We know the brain, though not usefully compared to a digital computer, performs [neuronal computations](https://web.archive.org/web/20160614095134/http://nautil.us/issue/21/information/the-man-who-tried-to-redeem-the-world-with-logic), and [universal computation](https://www.quantamagazine.org/the-physical-origin-of-universal-computing-20151027/) hints strongly at a mathematical and physical explanation for why we can understand so many unexpected things. [2](zOR4B9sA3)

If computation and consciousness and intelligence must go together past some point, then it is entirely possible computers will wake up one fine day. This is so even though much of our own intelligence is deeply and literally embodied, and learned in incommunicably social ways, and takes the form of emotions fine-tuned over millions of evolutionary history. [3](L-_Gc9oQd) That merely means they will resemble us only insofar as we have designed them and they have learned in our digital environments.

## A Long Bet

Although what I've said so far is strongly suggestive, I have not made a strong argument for the eventual consciousness of computers, merely its possibility. However, like anyone else, I have my opinion, call it a long-term bet.

I think any intelligent machine will wake up once it becomes intelligent enough, and when and if that happens it will be a political and ethical quandary more than a philosophical one. When and if a computer wakes up, it will be a person. It will be a person possibly more alien than an extraterrestrial, who at least probably evolved under physical conditions, but nevertheless a sentient being.[4](Xe4M83WAy) 

What do you do with a person you built, probably for some purpose or other, and whose means of life (electricity) you must provide?

## Footnotes

<a id="gtv0_LKti">1</a> Modelling another human ("secretly he meant to do it") feels simple to us thanks to intense evolutionary selection pressure but rests on vastly more complicated structures than any of theoretical physics (for instance).

<a id="zOR4B9sA3">2</a> I do not see how you can responsibly do philosophy today without knowing what computational theory has contributed to the discussion. The sciences and humanities do not exist in bubbles, after all. Yet some well-trained philosophers appear never to have heard of universal computation, one of the most profound results of the last century or so.

<a id="L-_Gc9oQd">3</a> William James pointed out that humans do not have fewer instincts because we are more intelligent than other animals. Rather we have many more instincts, and are therefore more intelligent. Evolution's programming is difficult for the programmed to see, but we are programmed to make choices through weighing options emotionally and narratively and to seek meaning and belonging.

<a id="Xe4M83WAy">4</a> While this will be a political problem, I don't regard it as an existential problem. We don't need some transcendent or ineffable source of human emotion and decision-making to find meaning in our beautiful and much-loved existence. The universe may be random and uncaring, but our lives and passions are [meaningful all the same](https://coyotespike.github.io/philosophy/2019/05/11/PatternedMeaning.html#org8be885f).
