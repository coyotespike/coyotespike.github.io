#+OPTIONS: toc:nil
#+BEGIN_HTML
---
layout: post
title: Wetware is Hard
date: 2016-04-11 14:19
# Should be longer than 150 words, so it will appear as post summary
description: "The life sciences carry incredible promise, but many bright researchers avoid it for faster-moving fields with better tools."
# tags will also be used as html meta keywords.
tags:
  - synthetic biology

show_meta: true
comments: true
mathjax: true
gistembed: true
published: true
noindex: false
nofollow: false
---
#+END_HTML
#+: toc:4

Let's talk about a couple of problems with life science that we can see by comparing
it with the rapidly-innovating field of computer science. The Silicon Valley mantra
is "hardware is hard," and we know that wetware is even harder. Biology is slow
and hard.

Nevertheless, and while admitting a possibly-na√Øve viewpoint, the get-it-done
impatience of computer science shows us a couple of problems in the life sciences.

*** 1. Computation, Software, Automation
Life science largely proceeds by hand and stores results in paper notebook.
We have pipette robots but few others. The default is to store the vast amounts of
data generated on the long trail to a published paper in hard-to-access 
and hard-to-share forms.

The cutting edge is Excel worksheets. 

*** 2. Reproducibility
From Ben Goldacre and others we're familiar with the problem of positive publication
bias. A positive result is twice as likely to be published, and researchers monkey
with data sub-categories until they reach a significant result. They then restrict
access to their data so no one else can steal it.

Here we are talking about a different but related problem. While industry scientists
usually explicitly do not want others to know about their results, so they can commercialize
it, academic scientists claim to want to share their results, but actually publish just enough
to get the credit but not enough to easily reproduce the result.

Many scientists do not want to share their data or their methods. Why would they?
Another lab will have that data and also their own data and thereby gain a competitive advantage.
By keeping your techniques and data close to the chest, you can further develop your results
yourself, and get any awards and funding that accrue.

Just as bad, scientists don't document the various methods they tested en route to the final experiment.
Other fields proceed much more quickly by documenting and sharing "failure modes,"
tested protocols that don't work.

In the life sciences we have to look up and remember old experiments, how they
were done, and how not to do them.

*** Let's contrast this dismal picture by an idealized one.
You design your experiment using a piece of software that (1) looks at past failure
modes so it knows what not to do, (2) tells you what reagents/materials and lab workflow to use,
(3) sends the experiment off to the cloud lab or to your in-lab robots, (4) documents
the experiment, materials, data, and failure modes, (5) after you publish, helps you
open-source all of this.

The actual and ideal states of affairs carry a number of implications.
*First*, life science lab work is painful, intrinsically so, despite any passion
one has for the process. Zero automated or programmatic techniques - robots or software
- are used by most labs. That doesn't sound fun.

*Second*, life science labs are conservative, very reluctant to incorporate new software
tools. They have research to produce now and don't want to retrofit. Again, not fun.

*Third*, a lab that talks openly and transparently about how it promotes its members'
careers, about how it's promoting open-source, could gain a competitive advantage
in recruiting talented colleagues.

*Fourth*, Sturgeon's law applies to science as to anywhere else: ninety percent of
everything is crap. The techniques are crap, the results are crap, the publication
is crap, the sharing is crap, everything is crap. Therefore, we cannot trust that
any research we want done will be done well.

*Fifth*, open-sourcing research the way software is carries significant group benefits,
but may carry individual risk. Other labs will use your results to get better results,
and you get no benefit from that. I believe some labs use informal contacts as a 
system to get feedback and credit prior to publication or more thorough data sharing.
A better system to accomplish the same goals might allow you to share more.

Alternatively, we can aggressively promote organizations and people who share this
open-source ideology might allow us to build a strong network. And we can use automation
and software and a no-frills approach to move faster, and to attract fast-moving
people, and thereby simply outrun most of the competition.

These problems with the state of science are as fundamental as known problems
like unconscious research bias and bad meta-studies. 
Science is broken and we are all going to die. We should fix it.
